{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8gZlVyCCbz"
      },
      "source": [
        "# Modelos del lenguaje con RNNs\n",
        "\n",
        "En esta parte, vamos a entrenar un modelo del lenguaje basado en caracteres con Recurrent Neural Networks. Asimismo, utilizaremos el modelo para generar texto. En particular, alimentaremos nuestro modelo con obras de la literatura cl√°sica en castellano para obtener una red neuronal que sea capaz de \"escribir\" fragmentos literarios.\n",
        "\n",
        "Los entrenamientos para obtener un modelo de calidad podr√≠an tomar cierto tiempo (5-10 minutos por epoch), por lo que se aconseja empezar a trabajar pronto. El uso de GPUs no ayuda tanto con LSTMs como con CNNs, por lo que si ten√©is m√°quinas potentes en casa es posible que pod√°is entrenar m√°s r√°pido o a la misma velocidad que en Colab. En todo caso, la potencia de Colab es m√°s que suficiente para completar esta actividad con √©xito.\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d8/El_ingenioso_hidalgo_don_Quijote_de_la_Mancha.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
        "\n",
        "El dataset a utilizar consistir√° en un archivo de texto con el contenido √≠ntegro en castellano antiguo de El Ingenioso Hidalgo Don Quijote de la Mancha, disponible de manera libre en la p√°gina de [Project Gutenberg](https://www.gutenberg.org). Asimismo, como apartado optativo en este laboratorio se pueden utilizar otras fuentes de texto. Aqu√≠ pod√©is descargar los datos a utilizar de El Quijote y un par de obras adicionales:\n",
        "\n",
        "[El ingenioso hidalgo Don Quijote de la Mancha (Miguel de Cervantes)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io)\n",
        "\n",
        "[Compilaci√≥n de obras teatrales (Calder√≥n de la Barca)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219433&authkey=AKvGD6DC3IRBqmc)\n",
        "\n",
        "[Trafalgar (Benito P√©rez Gald√≥s)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ)\n",
        "\n",
        "Como ya deber√≠amos de estar acostumbrados en problemas de Machine Learning, es importante echar un vistazo a los datos antes de empezar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI274F8LQC59"
      },
      "source": [
        "## 1. Carga y procesado del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZNnzvXuqVVm"
      },
      "source": [
        "Primero, vamos a descargar el libro e inspeccionar los datos. El fichero a descargar es una versi√≥n en .txt del libro de Don Quijote, a la cual se le han borrado introducciones, licencias y otras secciones para dejarlo con el contenido real de la novela."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "CZ62Ed0p9kRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7tKOZ9BFfki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a8f3e1-33c9-4029-e7ad-ee254723657f"
      },
      "source": [
        "path = keras.utils.get_file(\n",
        "    fname=\"don_quijote.txt\",\n",
        "    origin=\" https://raw.githubusercontent.com/JaznaLaProfe/Deep-Learning/main/textos/Don_Quijote_de_la_Mancha.txt\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from  https://raw.githubusercontent.com/JaznaLaProfe/Deep-Learning/main/textos/Don_Quijote_de_la_Mancha.txt\n",
            "\u001b[1m2151176/2151176\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGLvjLXrUUd"
      },
      "source": [
        "Una vez descargado, vamos a leer el contenido del fichero en una variable. Adicionalmente, convertiremos el contenido del texto a min√∫sculas para pon√©rselo un poco m√°s f√°cil a nuestro modelo (de modo que todas las letras sean min√∫sculas y el modelo no necesite diferenciar entre min√∫sculas y may√∫sculas).\n",
        "\n",
        "**1.1.** Leer todo el contenido del fichero en una √∫nica variable ***text*** y convertir el string a min√∫sculas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WB6FejrrTu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad258c8-9d24-4a13-be9d-348d3c43f658"
      },
      "source": [
        "# Abrir y leer el contenido\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    texto = f.read()\n",
        "\n",
        "# Mostrar los primeros 500 caracteres (por ejemplo)\n",
        "print(texto[:500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cap√≠tulo primero. Que trata de la condici√≥n y ejercicio del famoso hidalgo\n",
            "don Quijote de la Mancha\n",
            "\n",
            "\n",
            "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho\n",
            "tiempo que viv√≠a un hidalgo de los de lanza en astillero, adarga antigua,\n",
            "roc√≠n flaco y galgo corredor. Una olla de algo m√°s vaca que carnero,\n",
            "salpic√≥n las m√°s noches, duelos y quebrantos los s√°bados, lantejas los\n",
            "viernes, alg√∫n palomino de a√±adidura los domingos, consum√≠an las tres\n",
            "partes de su hacienda. El resto della co\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkgGl8GWtUk8"
      },
      "source": [
        "Podemos comprobar ahora que efectivamente nuestra variable contiene el resultado deseado, con el comienzo tan caracter√≠stico del Quijote."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMFhe3COFwSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbfc991-b5c8-446a-d46f-f46f5c3dcc08"
      },
      "source": [
        "print(\"Longitud del texto: {}\".format(len(texto)))\n",
        "print(texto[0:300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del texto: 2071198\n",
            "Cap√≠tulo primero. Que trata de la condici√≥n y ejercicio del famoso hidalgo\n",
            "don Quijote de la Mancha\n",
            "\n",
            "\n",
            "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho\n",
            "tiempo que viv√≠a un hidalgo de los de lanza en astillero, adarga antigua,\n",
            "roc√≠n flaco y galgo corredor. Una olla de algo m√°s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ7TUXWiyvOj"
      },
      "source": [
        "## 2. Procesado de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x66_Vi_Gyxns"
      },
      "source": [
        "Una de las grandes ventajas de trabajar con modelos que utilizan caracteres en vez de palabras es que no necesitamos tokenizar el texto (partirlo palabra a palabra). Nuestro modelo funcionar√° directamente con los caracteres en el texto, incluyendo espacios, saltos de l√≠nea, etc.\n",
        "\n",
        "Antes de hacer nada, necesitamos procesar el texto en entradas y salidas compatibles con nuestro modelo. Como sabemos, un modelo del lenguaje con RNNs acepta una serie de caracteres y predice el siguiente car√°cter en la secuencia.\n",
        "\n",
        "* \"*El ingenioso don Qui*\" -> predicci√≥n: **j**\n",
        "* \"*El ingenioso don Quij*\" -> predicci√≥n: **o**\n",
        "\n",
        "De modo que la entrada y la salida de nuestro modelo necesita ser algo parecido a este esquema. En este punto, podr√≠amos usar dos formas de preparar los datos para nuestro modelo.\n",
        "\n",
        "1. **Secuencia a secuencia**. La entrada de nuestro modelo ser√≠a una secuencia y la salida ser√≠a esa secuencia trasladada un caracter a la derecha, de modo que en cada instante de tiempo la RNN tiene que predecir el car√°cter siguiente. Por ejemplo:\n",
        "\n",
        ">* *Input*:   El ingenioso don Quijot\n",
        ">* *Output*: l ingenioso don Quijote\n",
        "\n",
        "2. **Secuencia a car√°cter**. En este variante, pasar√≠amos una secuencia de caracteres por nuestra RNN y, al llegar al final de la secuencia, predecir√≠amos el siguiente car√°cter.\n",
        "\n",
        ">* *Input*:   El ingenioso don Quijot\n",
        ">* *Output*: e\n",
        "\n",
        "En este laboratorio, por simplicidad, vamos a utilizar la segunda variante.\n",
        "\n",
        "De este modo, a partir del texto, hemos de generar nuestro propio training data que consista en secuencias de caracteres con el siguiente car√°cter a predecir. Para estandarizar las cosas, utilizaremos secuencias de tama√±o *SEQ_LENGTH* caracteres (un hiperpar√°metro que podemos elegir nosotros).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfJUIxW5m5C"
      },
      "source": [
        "#### 2.1. Obtenci√≥n de los caracteres y mapas de caracteres\n",
        "\n",
        "Antes que nada, necesitamos saber qu√© caracteres aparecen en el texto, ya que tendremos que diferenciarlos mediante un √≠ndice de 0 a *num_chars* - 1 en el modelo. Obtener:\n",
        "\n",
        "\n",
        "1.   N√∫mero de caracteres √∫nicos que aparecen en el texto.\n",
        "2.   Diccionario que asocia char a √≠ndice √∫nico entre 0 y *num_chars* - 1. Por ejemplo, {'a': 0, 'b': 1, ...}\n",
        "3.   Diccionario reverso de √≠ndices a caracteres: {0: 'a', 1: 'b', ...}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bJ0NsbCbupF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1564117-319f-4352-da93-d7161bf2377c"
      },
      "source": [
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Eliminar acentos (pero preservar la √± manualmente)\n",
        "    texto = texto.replace('√±', '__enie__')  # marcador temporal\n",
        "    texto_normalizado = unicodedata.normalize('NFD', texto)\n",
        "    texto_sin_acentos = ''.join(c for c in texto_normalizado if unicodedata.category(c) != 'Mn')\n",
        "    texto_sin_acentos = texto_sin_acentos.replace('__enie__', '√±')  # restaurar √±\n",
        "\n",
        "    # Eliminar caracteres especiales, dejar solo letras, n√∫meros, espacios y √±\n",
        "    texto_limpio = re.sub(r'[^a-z0-9√± ]+', '', texto_sin_acentos)\n",
        "\n",
        "    return texto_limpio\n",
        "\n",
        "\n",
        "resultado = limpiar_texto(texto)\n",
        "\n",
        "\n",
        "# Obtener el conjunto de caracteres √∫nicos\n",
        "chars = sorted(list(set(resultado)))\n",
        "\n",
        "# N√∫mero de caracteres √∫nicos\n",
        "num_chars = len(chars)\n",
        "print(f\"N√∫mero de caracteres √∫nicos: {num_chars}\")\n",
        "\n",
        "# Crear diccionario de caracteres a √≠ndices\n",
        "char_to_index = {char: index for index, char in enumerate(chars)}\n",
        "\n",
        "# Crear diccionario de √≠ndices a caracteres\n",
        "index_to_char = {index: char for index, char in enumerate(chars)}\n",
        "\n",
        "# Imprimir algunos ejemplos para verificar\n",
        "print(\"\\nEjemplo de char_to_index:\")\n",
        "for i in range(len(char_to_index)):\n",
        "    print(f\"'{chars[i]}': {char_to_index[chars[i]]}\")\n",
        "\n",
        "print(\"\\nEjemplo de index_to_char:\")\n",
        "for i in range(len(char_to_index)):\n",
        "    print(f\"{i}: '{index_to_char[i]}'\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N√∫mero de caracteres √∫nicos: 35\n",
            "\n",
            "Ejemplo de char_to_index:\n",
            "' ': 0\n",
            "'0': 1\n",
            "'1': 2\n",
            "'2': 3\n",
            "'3': 4\n",
            "'4': 5\n",
            "'5': 6\n",
            "'6': 7\n",
            "'7': 8\n",
            "'a': 9\n",
            "'b': 10\n",
            "'c': 11\n",
            "'d': 12\n",
            "'e': 13\n",
            "'f': 14\n",
            "'g': 15\n",
            "'h': 16\n",
            "'i': 17\n",
            "'j': 18\n",
            "'l': 19\n",
            "'m': 20\n",
            "'n': 21\n",
            "'o': 22\n",
            "'p': 23\n",
            "'q': 24\n",
            "'r': 25\n",
            "'s': 26\n",
            "'t': 27\n",
            "'u': 28\n",
            "'v': 29\n",
            "'w': 30\n",
            "'x': 31\n",
            "'y': 32\n",
            "'z': 33\n",
            "'√±': 34\n",
            "\n",
            "Ejemplo de index_to_char:\n",
            "0: ' '\n",
            "1: '0'\n",
            "2: '1'\n",
            "3: '2'\n",
            "4: '3'\n",
            "5: '4'\n",
            "6: '5'\n",
            "7: '6'\n",
            "8: '7'\n",
            "9: 'a'\n",
            "10: 'b'\n",
            "11: 'c'\n",
            "12: 'd'\n",
            "13: 'e'\n",
            "14: 'f'\n",
            "15: 'g'\n",
            "16: 'h'\n",
            "17: 'i'\n",
            "18: 'j'\n",
            "19: 'l'\n",
            "20: 'm'\n",
            "21: 'n'\n",
            "22: 'o'\n",
            "23: 'p'\n",
            "24: 'q'\n",
            "25: 'r'\n",
            "26: 's'\n",
            "27: 't'\n",
            "28: 'u'\n",
            "29: 'v'\n",
            "30: 'w'\n",
            "31: 'x'\n",
            "32: 'y'\n",
            "33: 'z'\n",
            "34: '√±'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este bloque se genera una funci√≥n que se encarga de limpiar el texto para poder mentener unicamente los caracteres necesarios para completar el texto, se realizaron los siguientes cambios.\n",
        "\n",
        "- Se utilizo normalizacion utilizando unicode en formato NFD, esto lo que hace es separar las tildes y caracteres especiales presentes en una letra, ejemplo: √° ‚Üí a ¬¥.\n",
        "\n",
        "- Teniendo en cuenta el punto anterior, la letra √ë se mantuvo por lo que se transformo de manera temporal a __enie__ para evitar que el formato unicode NFD separe la virgulilla y transforme la \"√ë\" a una \"N ~\".\n",
        "\n",
        "- Se eliminaron todos los signos de puntuacion, pregunta, exclamacion, etc. Solo se mantuvieron las letras del abecedario, numeros, espacios en blanco y la letra \"√ë\" que si bien puede tomarse como un carater especial, es considerada una letra.\n",
        "\n",
        "Finalmente cuando se realiza la limpieza se generan los caracteres de indice a caracteres y caracter a indice para podere valuar como quedarian los datos, con un valor numerico entero para poder realizar el entrenamiento."
      ],
      "metadata": {
        "id": "x7h7hSb_6yJb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_B4AWo0ElwA"
      },
      "source": [
        "#### 2.2. Obtenci√≥n de secuencias de entrada y car√°cter a predecir\n",
        "\n",
        "Ahora, vamos a obtener las secuencias de entrada en formato texto y los correspondientes caracteres a predecir. Para ello, recorrer el texto completo le√≠do anteriormente, obteniendo una secuencia de SEQ_LENGTH caracteres y el siguiente caracter a predecir. Una vez hecho, desplazarse un car√°cter a la izquierda y hacer lo mismo para obtener una nueva secuencia y predicci√≥n. Guardar las secuencias en una variable ***sequences*** y los caracteres a predecir en una variable ***next_chars***.\n",
        "\n",
        "Por ejemplo, si el texto fuera \"Don Quijote\" y SEQ_LENGTH fuese 5, tendr√≠amos\n",
        "\n",
        "* *sequences* = [\"Don Q\", \"on Qu\", \"n Qui\", \" Quij\", \"Quijo\", \"uijot\"]\n",
        "* *next_chars* = ['u', 'i', 'j', 'o', 't', 'e']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NslxhnnDK6uA"
      },
      "source": [
        "# Definimos el tama√±o de las secuencias. Puedes dejar este valor por defecto.\n",
        "SEQ_LENGTH = 60\n",
        "\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "# Recorremos el texto y extraemos las secuencias de longitud fija y el siguiente car√°cter\n",
        "for i in range(0, len(resultado) - SEQ_LENGTH):\n",
        "    sequences.append(resultado[i: i + SEQ_LENGTH])     # secuencia de 30 caracteres\n",
        "    next_chars.append(resultado[i + SEQ_LENGTH])        # siguiente car√°cter despu√©s de la secuencia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manteniendo la explicacion principal, se generan secuencias con 60 caracteres en la variable secuences, que siempre sera una secuencia cortada donde falte 1 caracter, en la variable next_chars estara almacenado el caracter faltante de la secuencia. Esto es un enfoque tmado en clasificacion donde secuences es el valor de X y next_chars funciona como la Y (Valor de la clase o resultado a predecir)."
      ],
      "metadata": {
        "id": "SOZ4J0ZLHRlk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y3AmjYtHdLJ"
      },
      "source": [
        "Indicar el tama√±o del training set que acabamos de generar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVWqKxFcbwTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe09e677-deae-4870-e41e-c89c8a7df001"
      },
      "source": [
        "print(f\"N√∫mero de secuencias: {len(sequences)}\")\n",
        "print(f\"N√∫mero de caracteres objetivo: {len(next_chars)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N√∫mero de secuencias: 1968193\n",
            "N√∫mero de caracteres objetivo: 1968193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al generar las secuencias se generaron, a partir del texto, 1.968.193 secuencias diferentes, mismo numero de carateres o clases (Y)."
      ],
      "metadata": {
        "id": "K96YTIzVHtj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def es_secuencia_valida(seq):\n",
        "    # Si es todo espacios o una sola letra repetida, no sirve\n",
        "    return seq.strip() != \"\" and len(set(seq)) > 1\n",
        "\n",
        "# Aplicar filtro\n",
        "sequences_filtradas = []\n",
        "next_chars_filtradas = []\n",
        "\n",
        "for seq, next_char in zip(sequences, next_chars):\n",
        "    if es_secuencia_valida(seq):\n",
        "        sequences_filtradas.append(seq)\n",
        "        next_chars_filtradas.append(next_char)\n",
        "\n",
        "print(f\"Secuencias √∫tiles: {len(sequences_filtradas)} / {len(sequences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLBIgOe9RGyo",
        "outputId": "097e8f76-ac62-4117-db90-9e75e7e83c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secuencias √∫tiles: 1968193 / 1968193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui se evalua la presencia de secuencias inutiles o que no entregan nada de relevancia al modelo como por ejemplo: Ahhhhh, que esto. En este caso no ocurre por el tama√±o seteado de las secuencias (60 caracteres) lo cual impide secuencias asi, tal como se ve se mantuvieron todas las sencuencias y no se descarto ninguna."
      ],
      "metadata": {
        "id": "EOUyX1f0Ibl8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goGQkKcwpLRJ"
      },
      "source": [
        "Como el Quijote es muy largo y tenemos muchas secuencias, podr√≠amos encontrar problemas de memoria. Por ello, vamos a elegir un n√∫mero m√°ximo de ellas. Si est√°s corriendo esto localmente y tienes problemas de memoria, puedes reducir el tama√±o a√∫n m√°s, pero ten cuidado porque, a menos datos, peor calidad del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pm1Q19ppw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7e8f9c-dd93-46f6-9ebc-3a369fb1ab2c"
      },
      "source": [
        "MAX_SEQUENCES = 350000\n",
        "\n",
        "sequences = np.array(sequences_filtradas)\n",
        "next_chars = np.array(next_chars_filtradas)\n",
        "\n",
        "perm = np.random.permutation(len(sequences_filtradas))\n",
        "sequences, next_chars = sequences[perm], next_chars[perm]\n",
        "sequences, next_chars = np.array(sequences), np.array(next_chars)\n",
        "sequences, next_chars = sequences[perm], next_chars[perm]\n",
        "sequences, next_chars = list(sequences[:MAX_SEQUENCES]), list(next_chars[:MAX_SEQUENCES])\n",
        "\n",
        "print(len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "350000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por factores de consumo de recursos de Google colab, no se utilizaran el 1.968.193 datos, sino que se utilizara una fraccion de estos, especificamente 350.000, donde el codigo genera con la permutacion un bajareo de indices de las secuencias y sus siguientes carateres, esto para tomar los 350.000 datos de manera aleatoria."
      ],
      "metadata": {
        "id": "hGNdxljAIuCg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FzgtAbPIs6f"
      },
      "source": [
        "#### 2.3. Obtenci√≥n de input X y output y para el modelo\n",
        "\n",
        "Finalmente, a partir de los datos de entrenamiento que hemos generado vamos a crear los arrays de datos X e y que pasaremos a nuestro modelo.\n",
        "\n",
        "Para ello, vamos a utilizar *one-hot encoding* para nuestros caracteres. Por ejemplo, si s√≥lo tuvi√©ramos 4 caracteres (a, b, c, d), las representaciones ser√≠an: (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0) y (0, 0, 0, 1).\n",
        "\n",
        "De este modo, **X** tendr√° shape *(num_sequences, seq_length, num_chars)* e **y** tendr√° shape *(num_sequences, num_chars)*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMBwZ9obNGNg"
      },
      "source": [
        "NUM_CHARS = 35  # Tu n√∫mero de caracteres distintos aqu√≠\n",
        "NUM_SEQUENCES = len(sequences)\n",
        "X = np.zeros((NUM_SEQUENCES, SEQ_LENGTH), dtype=np.int32)\n",
        "y = np.zeros((NUM_SEQUENCES,), dtype=np.int32)\n",
        "\n",
        "\n",
        "for i, seq in enumerate(sequences):\n",
        "    for t, char in enumerate(seq):\n",
        "        X[i, t] = char_to_index[char]\n",
        "    y[i] = char_to_index[next_chars[i]]  # siguiente caracter (1 por secuencia)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este codigo genera nuestra separacion de datos en X e Y, utiliza el arreglo de caracteres a indices para guardar la secuencia en X (de manera numerica con el indice entero) y su siguiente carater en Y con el valor numerico ya listo para pasarse al modelo."
      ],
      "metadata": {
        "id": "obbTIHhUMbMs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeUxz3HPm3l"
      },
      "source": [
        "## 3. Definici√≥n del modelo y entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se tomo punto base el crear la LSTM con 128 unidades internas, utilizando softmax. Pero la arquitectura de la RNN se fue complejizando hasta dejar una arquitectura y preprocesamiento de datos mas completo buscando mejorar el rendimiento del modelo. Las tecnicas aplicadas como preprocesamiento y definicion del modelo son las siguientes:\n",
        "\n",
        "- Generacion de pesos artificiales: Esto para poder sustentar un poco el desbalance de datos, cuya existencia en un set de datos no estructurado es casi inevitable.\n",
        "\n",
        "- Aplicacion de Early Stopping: Debido al alto consumo de recursos para la RNN se aplico early stopping para evitar que el modelo siga consumiendo recursos en epocas que solo estan empeorando o manteniendo el mismo rendimiento.\n",
        "\n",
        "- Uso de LSTM junto a bidirectional con 512 unidades de memoria: Para mejorar el rendimiento general del modelo se aplica una capa LSTM con bidireccional para que el modelo procese la secuencia de entrada de izquierda a derecha (como loa hace la LSTM tradicional) pero a su vez aplique un analisis de derecha a izquierda. Se apicaron 512 unidades de memoria, en contra dle punto inicial debido al bajo rendimiento que se entrego con esa prueba."
      ],
      "metadata": {
        "id": "hSvPKrLBMrTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conteo = Counter(next_chars)\n",
        "total = sum(conteo.values())\n",
        "\n",
        "# Frecuencia relativa de cada car√°cter\n",
        "frecuencias = {char: count / total for char, count in conteo.items()}\n",
        "\n",
        "# Peso inverso para cada clase\n",
        "pesos = {char: 1.0 / freq for char, freq in frecuencias.items()}\n",
        "\n",
        "max_peso = max(pesos.values())\n",
        "pesos = {char: peso / max_peso for char, peso in pesos.items()}\n",
        "\n",
        "sample_weights = np.array([pesos[c] for c in next_chars], dtype=np.float32)"
      ],
      "metadata": {
        "id": "pHk_jZwhRdl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque de codigo genera pesos artificiales calculando la frecuencia del siguiente caracter usando Counter, con esto se aplican pesos inversos, es decir, los caracteres mas frecuentes se les genera un peso menor a los menos frecuentes buscando compensar el desbalance de clases claramente presente."
      ],
      "metadata": {
        "id": "wf5jC0s5QWQk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSw2j0btYWZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3375e760-f821-4f41-b365-0489b2e94fa1"
      },
      "source": [
        "embedding_dim = 512\n",
        "rnn_units = 512\n",
        "early_stopping = EarlyStopping(monitor=\"loss\", patience=5, min_delta=0.05, restore_best_weights=True)\n",
        "\n",
        "model_2 = Sequential([\n",
        "    Embedding(input_dim=NUM_CHARS, output_dim=embedding_dim),\n",
        "    Bidirectional(LSTM(rnn_units, return_sequences=True, dropout=0.2)),\n",
        "    Bidirectional(LSTM(rnn_units, return_sequences=False, dropout=0.2)),\n",
        "    Dense(NUM_CHARS, activation='softmax')\n",
        "])\n",
        "\n",
        "model_2.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy')\n",
        "model_2.fit(X, y, batch_size=64, epochs=15, callbacks=[early_stopping], sample_weight=sample_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 92ms/step - loss: 2.8719e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 93ms/step - loss: 2.1499e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 93ms/step - loss: 2.3667e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 93ms/step - loss: 2.1297e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 93ms/step - loss: 1.8381e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 93ms/step - loss: 1.7012e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c3f02f71a50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se aplico una RNN que se compone de las siguientes capas:\n",
        "\n",
        "- Capa de embedding con dimesion de 512: Esto tomara los numeros enteros (los indices de los caracteres) y seran aplanados en un vector de 512 dimensiones. Esto puede mejorar bastante el rendimiento del modelo pero el consumo de recursos es mucho mayor\n",
        "\n",
        "- 2 capas LSTM con bidirectional y 512 unidades de memoria (neuronas): Se utilizaron dos capas LSTM envoltas en capas Bidirectional, lo que permite que la red procese la informaci√≥n tanto en direcci√≥n hacia adelante como hacia atr√°s, capturando mejor el contexto en ambas direcciones. La primera capa devuelve una secuencia completa de caracter√≠sticas (una salida por cada paso de la secuencia), que luego es procesada por la segunda capa para extraer patrones m√°s complejos de lenguaje. Esto mejora la capacidad del modelo para aprender dependencias a largo plazo en el texto.\n",
        "\n",
        "- Capa densa de salida: La capa que nos entregara el siguiente caracter de la secuencia, como sucede en las redes de clasificacion tradicionales.\n",
        "\n",
        "- Funcion de perdida: Se aplica Sparse_categorical_crossentropy debido a que no se esta utilizando un formato de one hot encoder, se utiliza esa funcion de perdida.\n"
      ],
      "metadata": {
        "id": "veRvahgKQi62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_multiple_texts(model, start_phrases, gen_length=100, temperature=1.0):\n",
        "    for phrase in start_phrases:\n",
        "        input_indices = [char_to_index[c] for c in phrase.lower()]\n",
        "        input_seq = tf.expand_dims(input_indices, 0)\n",
        "        generated = phrase\n",
        "\n",
        "        for _ in range(gen_length):\n",
        "            preds = model(input_seq)  # √∫ltima predicci√≥n\n",
        "            preds = preds / temperature\n",
        "            next_id = tf.random.categorical(preds, num_samples=1).numpy()[0][0]\n",
        "            next_char = index_to_char[next_id]\n",
        "\n",
        "            generated += next_char\n",
        "            input_indices.append(next_id)\n",
        "            input_indices = input_indices[-SEQ_LENGTH:]\n",
        "            input_seq = tf.expand_dims(input_indices, 0)\n",
        "\n",
        "        print(f\"\\nüü° Frase inicial: '{phrase}'\")\n",
        "        print(f\"üîπ Predicci√≥n: {generated}\")"
      ],
      "metadata": {
        "id": "QZz072k6E4vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este c√≥digo define una funci√≥n llamada generate_multiple_texts que se encarga de generar texto autom√°ticamente usando un modelo de red neuronal entrenado. Lo que hace, explicado en lenguaje natural, es lo siguiente:\n",
        "\n",
        "La funci√≥n toma una lista de frases iniciales (start_phrases) y, para cada una, empieza a predecir y generar texto car√°cter por car√°cter. Primero convierte la frase de entrada en una secuencia de n√∫meros (√≠ndices de caracteres) que el modelo pueda entender. Luego, mientras no se alcance la cantidad deseada de caracteres (gen_length), la funci√≥n alimenta esa secuencia al modelo, que devuelve una probabilidad para cada posible car√°cter siguiente.\n",
        "\n",
        "Estas probabilidades se ajustan con un valor llamado temperature, que controla qu√© tan creativas o seguras son las predicciones (valores bajos = m√°s conservadoras; valores altos = m√°s creativas y variadas). Despu√©s, se escoge un car√°cter de forma aleatoria pero ponderada seg√∫n esas probabilidades, se agrega a la secuencia y se repite el proceso, desplazando la ventana de entrada para siempre usar los √∫ltimos caracteres generados.\n",
        "\n",
        "Al final, se imprime tanto la frase original como el texto generado autom√°ticamente por el modelo, mostrando c√≥mo contin√∫a la red neuronal esa idea inicial."
      ],
      "metadata": {
        "id": "_1MNKlU1zFAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frases_prueba = [\n",
        "    \"don quijote \",\n",
        "    \"en un lugar de la \",\n",
        "    \"no ha mucho tiempo \",\n",
        "    \"vivia un hidalgo \",\n",
        "    \"de los de lanza en \",\n",
        "    \"era de complexion \",\n",
        "    \"el hidalgo salio al \",\n",
        "    \"andaba por los caminos \",\n",
        "    \"una ma√±ana decidio \"\n",
        "]\n",
        "\n",
        "generate_multiple_texts(model_2, frases_prueba, gen_length=60, temperature=0.5)"
      ],
      "metadata": {
        "id": "lesg4Q9F4Fys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8d46ba-dd4a-49bb-e794-7592d785dd77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üü° Frase inicial: 'don quijote '\n",
            "üîπ Predicci√≥n: don quijote s1fgyavcyt5w1lmicgb7lgorcgxbl56qamfuoazummv6so0d1xjv0 vmoe√±w\n",
            "\n",
            "üü° Frase inicial: 'en un lugar de la '\n",
            "üîπ Predicci√≥n: en un lugar de la quubgy11√±uujzou7v√±pbd25da√±oqs6l00ozdn3gwz2x0wecggmjs2yn√±ev w\n",
            "\n",
            "üü° Frase inicial: 'no ha mucho tiempo '\n",
            "üîπ Predicci√≥n: no ha mucho tiempo pjhltlclyba4u2d72icz45ife34rafpantdnaf0rnn27qltoq0mocg36jdod\n",
            "\n",
            "üü° Frase inicial: 'vivia un hidalgo '\n",
            "üîπ Predicci√≥n: vivia un hidalgo en2y3fu4rhfj√±yrj0e√±we svqu3xe7a7ayx607j1w0dsy v0c7 m4 31b45y\n",
            "\n",
            "üü° Frase inicial: 'de los de lanza en '\n",
            "üîπ Predicci√≥n: de los de lanza en ytglbyalqlwnqb1rxpanjgis√±r754grbotyzm 4 aqubn6wdi√±7ha6g4iryn\n",
            "\n",
            "üü° Frase inicial: 'era de complexion '\n",
            "üîπ Predicci√≥n: era de complexion a1r7ey√±aafllaf2c√±f0ren zpgtzeoivhl4bodo12 yyvsxedtuie5ap06wn\n",
            "\n",
            "üü° Frase inicial: 'el hidalgo salio al '\n",
            "üîπ Predicci√≥n: el hidalgo salio al 3lslquawbeoelq6rx4rczujhy n5a6hjbmnplxy7nrsteqogyyarb6wqc7j7\n",
            "\n",
            "üü° Frase inicial: 'andaba por los caminos '\n",
            "üîπ Predicci√≥n: andaba por los caminos 7u5√± m6z25qyv2lzzlcmr27√±luf3pri6z4topeu04i7fb7yrm6g6q√±zozwvo\n",
            "\n",
            "üü° Frase inicial: 'una ma√±ana decidio '\n",
            "üîπ Predicci√≥n: una ma√±ana decidio o3yrjir4fi1wd5qofsp√±eadu3al4mvyq√±wmm 4nt6i1q0wx4√±7zsrn1axsob\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui se le entrega una lista de frases a la funcion para que el model orealice multiples predicciones, esto permite evaluar de forma general y evitar evaluar el modelo con una frase que pueda llegar a ser compleja por el rendimiento del mismo."
      ],
      "metadata": {
        "id": "grq0vf2L0LOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusi√≥n"
      ],
      "metadata": {
        "id": "0V9KJzcg0Wqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido a los recursos limitados que posee el entorno de ejecucion el rendimiento general del modelo es bastante bajo llegando al punto de que las predicciones no forman una palabra coherente (lenguaje espa√±ol). Esto puede deberse a la capa de embedding, debido a que vectoriza los datos en una dimension de 512 se necesita un mayor consumo de recursos, los cuales no se poseen. Cabe destacar que se realizara un estudio tomando un enfoque diferente donde se pueda realizar un preprocesamiento de datos utilizando formato one-hot-encoding, lo cual permitira llevarlo a un enfoque mas de multi clases con matrices de 0 y 1, esto hara que la capa de embedding no sea necesaria y el modelo no tenga que consumir tantos recursos para procesarlo dando un margen de recursos para poder distribuir de mejor forma y mejorar asi el rendimiento del mismo.\n",
        "\n",
        "Enlace a investigacion con one-hot-encoding (Notebook 2):"
      ],
      "metadata": {
        "id": "eqTwF4pD0aIT"
      }
    }
  ]
}