{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8gZlVyCCbz"
      },
      "source": [
        "# Modelos del lenguaje con RNNs\n",
        "\n",
        "En esta parte, vamos a entrenar un modelo del lenguaje basado en caracteres con Recurrent Neural Networks. Asimismo, utilizaremos el modelo para generar texto. En particular, alimentaremos nuestro modelo con obras de la literatura clásica en castellano para obtener una red neuronal que sea capaz de \"escribir\" fragmentos literarios.\n",
        "\n",
        "Los entrenamientos para obtener un modelo de calidad podrían tomar cierto tiempo (5-10 minutos por epoch), por lo que se aconseja empezar a trabajar pronto. El uso de GPUs no ayuda tanto con LSTMs como con CNNs, por lo que si tenéis máquinas potentes en casa es posible que podáis entrenar más rápido o a la misma velocidad que en Colab. En todo caso, la potencia de Colab es más que suficiente para completar esta actividad con éxito.\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d8/El_ingenioso_hidalgo_don_Quijote_de_la_Mancha.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
        "\n",
        "El dataset a utilizar consistirá en un archivo de texto con el contenido íntegro en castellano antiguo de El Ingenioso Hidalgo Don Quijote de la Mancha, disponible de manera libre en la página de [Project Gutenberg](https://www.gutenberg.org). Asimismo, como apartado optativo en este laboratorio se pueden utilizar otras fuentes de texto. Aquí podéis descargar los datos a utilizar de El Quijote y un par de obras adicionales:\n",
        "\n",
        "[El ingenioso hidalgo Don Quijote de la Mancha (Miguel de Cervantes)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io)\n",
        "\n",
        "[Compilación de obras teatrales (Calderón de la Barca)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219433&authkey=AKvGD6DC3IRBqmc)\n",
        "\n",
        "[Trafalgar (Benito Pérez Galdós)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ)\n",
        "\n",
        "Como ya deberíamos de estar acostumbrados en problemas de Machine Learning, es importante echar un vistazo a los datos antes de empezar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI274F8LQC59"
      },
      "source": [
        "## 1. Carga y procesado del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZNnzvXuqVVm"
      },
      "source": [
        "Primero, vamos a descargar el libro e inspeccionar los datos. El fichero a descargar es una versión en .txt del libro de Don Quijote, a la cual se le han borrado introducciones, licencias y otras secciones para dejarlo con el contenido real de la novela."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "CZ62Ed0p9kRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7tKOZ9BFfki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a8f3e1-33c9-4029-e7ad-ee254723657f"
      },
      "source": [
        "path = keras.utils.get_file(\n",
        "    fname=\"don_quijote.txt\",\n",
        "    origin=\" https://raw.githubusercontent.com/JaznaLaProfe/Deep-Learning/main/textos/Don_Quijote_de_la_Mancha.txt\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from  https://raw.githubusercontent.com/JaznaLaProfe/Deep-Learning/main/textos/Don_Quijote_de_la_Mancha.txt\n",
            "\u001b[1m2151176/2151176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGLvjLXrUUd"
      },
      "source": [
        "Una vez descargado, vamos a leer el contenido del fichero en una variable. Adicionalmente, convertiremos el contenido del texto a minúsculas para ponérselo un poco más fácil a nuestro modelo (de modo que todas las letras sean minúsculas y el modelo no necesite diferenciar entre minúsculas y mayúsculas).\n",
        "\n",
        "**1.1.** Leer todo el contenido del fichero en una única variable ***text*** y convertir el string a minúsculas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WB6FejrrTu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad258c8-9d24-4a13-be9d-348d3c43f658"
      },
      "source": [
        "# Abrir y leer el contenido\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    texto = f.read()\n",
        "\n",
        "# Mostrar los primeros 500 caracteres (por ejemplo)\n",
        "print(texto[:500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Capítulo primero. Que trata de la condición y ejercicio del famoso hidalgo\n",
            "don Quijote de la Mancha\n",
            "\n",
            "\n",
            "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho\n",
            "tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua,\n",
            "rocín flaco y galgo corredor. Una olla de algo más vaca que carnero,\n",
            "salpicón las más noches, duelos y quebrantos los sábados, lantejas los\n",
            "viernes, algún palomino de añadidura los domingos, consumían las tres\n",
            "partes de su hacienda. El resto della co\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkgGl8GWtUk8"
      },
      "source": [
        "Podemos comprobar ahora que efectivamente nuestra variable contiene el resultado deseado, con el comienzo tan característico del Quijote."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMFhe3COFwSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbfc991-b5c8-446a-d46f-f46f5c3dcc08"
      },
      "source": [
        "print(\"Longitud del texto: {}\".format(len(texto)))\n",
        "print(texto[0:300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del texto: 2071198\n",
            "Capítulo primero. Que trata de la condición y ejercicio del famoso hidalgo\n",
            "don Quijote de la Mancha\n",
            "\n",
            "\n",
            "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho\n",
            "tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua,\n",
            "rocín flaco y galgo corredor. Una olla de algo más\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ7TUXWiyvOj"
      },
      "source": [
        "## 2. Procesado de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x66_Vi_Gyxns"
      },
      "source": [
        "Una de las grandes ventajas de trabajar con modelos que utilizan caracteres en vez de palabras es que no necesitamos tokenizar el texto (partirlo palabra a palabra). Nuestro modelo funcionará directamente con los caracteres en el texto, incluyendo espacios, saltos de línea, etc.\n",
        "\n",
        "Antes de hacer nada, necesitamos procesar el texto en entradas y salidas compatibles con nuestro modelo. Como sabemos, un modelo del lenguaje con RNNs acepta una serie de caracteres y predice el siguiente carácter en la secuencia.\n",
        "\n",
        "* \"*El ingenioso don Qui*\" -> predicción: **j**\n",
        "* \"*El ingenioso don Quij*\" -> predicción: **o**\n",
        "\n",
        "De modo que la entrada y la salida de nuestro modelo necesita ser algo parecido a este esquema. En este punto, podríamos usar dos formas de preparar los datos para nuestro modelo.\n",
        "\n",
        "1. **Secuencia a secuencia**. La entrada de nuestro modelo sería una secuencia y la salida sería esa secuencia trasladada un caracter a la derecha, de modo que en cada instante de tiempo la RNN tiene que predecir el carácter siguiente. Por ejemplo:\n",
        "\n",
        ">* *Input*:   El ingenioso don Quijot\n",
        ">* *Output*: l ingenioso don Quijote\n",
        "\n",
        "2. **Secuencia a carácter**. En este variante, pasaríamos una secuencia de caracteres por nuestra RNN y, al llegar al final de la secuencia, predeciríamos el siguiente carácter.\n",
        "\n",
        ">* *Input*:   El ingenioso don Quijot\n",
        ">* *Output*: e\n",
        "\n",
        "En este laboratorio, por simplicidad, vamos a utilizar la segunda variante.\n",
        "\n",
        "De este modo, a partir del texto, hemos de generar nuestro propio training data que consista en secuencias de caracteres con el siguiente carácter a predecir. Para estandarizar las cosas, utilizaremos secuencias de tamaño *SEQ_LENGTH* caracteres (un hiperparámetro que podemos elegir nosotros).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfJUIxW5m5C"
      },
      "source": [
        "#### 2.1. Obtención de los caracteres y mapas de caracteres\n",
        "\n",
        "Antes que nada, necesitamos saber qué caracteres aparecen en el texto, ya que tendremos que diferenciarlos mediante un índice de 0 a *num_chars* - 1 en el modelo. Obtener:\n",
        "\n",
        "\n",
        "1.   Número de caracteres únicos que aparecen en el texto.\n",
        "2.   Diccionario que asocia char a índice único entre 0 y *num_chars* - 1. Por ejemplo, {'a': 0, 'b': 1, ...}\n",
        "3.   Diccionario reverso de índices a caracteres: {0: 'a', 1: 'b', ...}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bJ0NsbCbupF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1564117-319f-4352-da93-d7161bf2377c"
      },
      "source": [
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Eliminar acentos (pero preservar la ñ manualmente)\n",
        "    texto = texto.replace('ñ', '__enie__')  # marcador temporal\n",
        "    texto_normalizado = unicodedata.normalize('NFD', texto)\n",
        "    texto_sin_acentos = ''.join(c for c in texto_normalizado if unicodedata.category(c) != 'Mn')\n",
        "    texto_sin_acentos = texto_sin_acentos.replace('__enie__', 'ñ')  # restaurar ñ\n",
        "\n",
        "    # Eliminar caracteres especiales, dejar solo letras, números, espacios y ñ\n",
        "    texto_limpio = re.sub(r'[^a-z0-9ñ ]+', '', texto_sin_acentos)\n",
        "\n",
        "    return texto_limpio\n",
        "\n",
        "\n",
        "resultado = limpiar_texto(texto)\n",
        "\n",
        "\n",
        "# Obtener el conjunto de caracteres únicos\n",
        "chars = sorted(list(set(resultado)))\n",
        "\n",
        "# Número de caracteres únicos\n",
        "num_chars = len(chars)\n",
        "print(f\"Número de caracteres únicos: {num_chars}\")\n",
        "\n",
        "# Crear diccionario de caracteres a índices\n",
        "char_to_index = {char: index for index, char in enumerate(chars)}\n",
        "\n",
        "# Crear diccionario de índices a caracteres\n",
        "index_to_char = {index: char for index, char in enumerate(chars)}\n",
        "\n",
        "# Imprimir algunos ejemplos para verificar\n",
        "print(\"\\nEjemplo de char_to_index:\")\n",
        "for i in range(len(char_to_index)):\n",
        "    print(f\"'{chars[i]}': {char_to_index[chars[i]]}\")\n",
        "\n",
        "print(\"\\nEjemplo de index_to_char:\")\n",
        "for i in range(len(char_to_index)):\n",
        "    print(f\"{i}: '{index_to_char[i]}'\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de caracteres únicos: 35\n",
            "\n",
            "Ejemplo de char_to_index:\n",
            "' ': 0\n",
            "'0': 1\n",
            "'1': 2\n",
            "'2': 3\n",
            "'3': 4\n",
            "'4': 5\n",
            "'5': 6\n",
            "'6': 7\n",
            "'7': 8\n",
            "'a': 9\n",
            "'b': 10\n",
            "'c': 11\n",
            "'d': 12\n",
            "'e': 13\n",
            "'f': 14\n",
            "'g': 15\n",
            "'h': 16\n",
            "'i': 17\n",
            "'j': 18\n",
            "'l': 19\n",
            "'m': 20\n",
            "'n': 21\n",
            "'o': 22\n",
            "'p': 23\n",
            "'q': 24\n",
            "'r': 25\n",
            "'s': 26\n",
            "'t': 27\n",
            "'u': 28\n",
            "'v': 29\n",
            "'w': 30\n",
            "'x': 31\n",
            "'y': 32\n",
            "'z': 33\n",
            "'ñ': 34\n",
            "\n",
            "Ejemplo de index_to_char:\n",
            "0: ' '\n",
            "1: '0'\n",
            "2: '1'\n",
            "3: '2'\n",
            "4: '3'\n",
            "5: '4'\n",
            "6: '5'\n",
            "7: '6'\n",
            "8: '7'\n",
            "9: 'a'\n",
            "10: 'b'\n",
            "11: 'c'\n",
            "12: 'd'\n",
            "13: 'e'\n",
            "14: 'f'\n",
            "15: 'g'\n",
            "16: 'h'\n",
            "17: 'i'\n",
            "18: 'j'\n",
            "19: 'l'\n",
            "20: 'm'\n",
            "21: 'n'\n",
            "22: 'o'\n",
            "23: 'p'\n",
            "24: 'q'\n",
            "25: 'r'\n",
            "26: 's'\n",
            "27: 't'\n",
            "28: 'u'\n",
            "29: 'v'\n",
            "30: 'w'\n",
            "31: 'x'\n",
            "32: 'y'\n",
            "33: 'z'\n",
            "34: 'ñ'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este bloque se genera una función que se encarga de limpiar el texto para poder mentener unicamente los caracteres necesarios para completar el texto, se realizaron los siguientes cambios.\n",
        "\n",
        "- Se utilizo normalizacion utilizando unicode en formato NFD, esto lo que hace es separar las tildes y caracteres especiales presentes en una letra, ejemplo: á → a ´.\n",
        "\n",
        "- Teniendo en cuenta el punto anterior, la letra Ñ se mantuvo por lo que se transformo de manera temporal a __enie__ para evitar que el formato unicode NFD separe la virgulilla y transforme la \"Ñ\" a una \"N ~\".\n",
        "\n",
        "- Se eliminaron todos los signos de puntuacion, pregunta, exclamacion, etc. Solo se mantuvieron las letras del abecedario, numeros, espacios en blanco y la letra \"Ñ\" que si bien puede tomarse como un carater especial, es considerada una letra.\n",
        "\n",
        "Finalmente cuando se realiza la limpieza se generan los caracteres de indice a caracteres y caracter a indice para podere valuar como quedarian los datos, con un valor numerico entero para poder realizar el entrenamiento."
      ],
      "metadata": {
        "id": "x7h7hSb_6yJb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_B4AWo0ElwA"
      },
      "source": [
        "#### 2.2. Obtención de secuencias de entrada y carácter a predecir\n",
        "\n",
        "Ahora, vamos a obtener las secuencias de entrada en formato texto y los correspondientes caracteres a predecir. Para ello, recorrer el texto completo leído anteriormente, obteniendo una secuencia de SEQ_LENGTH caracteres y el siguiente caracter a predecir. Una vez hecho, desplazarse un carácter a la izquierda y hacer lo mismo para obtener una nueva secuencia y predicción. Guardar las secuencias en una variable ***sequences*** y los caracteres a predecir en una variable ***next_chars***.\n",
        "\n",
        "Por ejemplo, si el texto fuera \"Don Quijote\" y SEQ_LENGTH fuese 5, tendríamos\n",
        "\n",
        "* *sequences* = [\"Don Q\", \"on Qu\", \"n Qui\", \" Quij\", \"Quijo\", \"uijot\"]\n",
        "* *next_chars* = ['u', 'i', 'j', 'o', 't', 'e']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NslxhnnDK6uA"
      },
      "source": [
        "# Definimos el tamaño de las secuencias. Puedes dejar este valor por defecto.\n",
        "SEQ_LENGTH = 60\n",
        "\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "# Recorremos el texto y extraemos las secuencias de longitud fija y el siguiente carácter\n",
        "for i in range(0, len(resultado) - SEQ_LENGTH):\n",
        "    sequences.append(resultado[i: i + SEQ_LENGTH])     # secuencia de 30 caracteres\n",
        "    next_chars.append(resultado[i + SEQ_LENGTH])        # siguiente carácter después de la secuencia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manteniendo la explicacion principal, se generan secuencias con 60 caracteres en la variable secuences, que siempre sera una secuencia cortada donde falte 1 caracter, en la variable next_chars estara almacenado el caracter faltante de la secuencia. Esto es un enfoque tmado en clasificacion donde secuences es el valor de X y next_chars funciona como la Y (Valor de la clase o resultado a predecir)."
      ],
      "metadata": {
        "id": "SOZ4J0ZLHRlk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y3AmjYtHdLJ"
      },
      "source": [
        "Indicar el tamaño del training set que acabamos de generar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVWqKxFcbwTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe09e677-deae-4870-e41e-c89c8a7df001"
      },
      "source": [
        "print(f\"Número de secuencias: {len(sequences)}\")\n",
        "print(f\"Número de caracteres objetivo: {len(next_chars)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de secuencias: 1968193\n",
            "Número de caracteres objetivo: 1968193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al generar las secuencias se generaron, a partir del texto, 1.968.193 secuencias diferentes, mismo numero de carateres o clases (Y)."
      ],
      "metadata": {
        "id": "K96YTIzVHtj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def es_secuencia_valida(seq):\n",
        "    # Si es todo espacios o una sola letra repetida, no sirve\n",
        "    return seq.strip() != \"\" and len(set(seq)) > 1\n",
        "\n",
        "# Aplicar filtro\n",
        "sequences_filtradas = []\n",
        "next_chars_filtradas = []\n",
        "\n",
        "for seq, next_char in zip(sequences, next_chars):\n",
        "    if es_secuencia_valida(seq):\n",
        "        sequences_filtradas.append(seq)\n",
        "        next_chars_filtradas.append(next_char)\n",
        "\n",
        "print(f\"Secuencias útiles: {len(sequences_filtradas)} / {len(sequences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLBIgOe9RGyo",
        "outputId": "097e8f76-ac62-4117-db90-9e75e7e83c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secuencias útiles: 1968193 / 1968193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui se evalua la presencia de secuencias inutiles o que no entregan nada de relevancia al modelo como por ejemplo: Ahhhhh, que esto. En este caso no ocurre por el tamaño seteado de las secuencias (60 caracteres) lo cual impide secuencias asi, tal como se ve se mantuvieron todas las sencuencias y no se descarto ninguna."
      ],
      "metadata": {
        "id": "EOUyX1f0Ibl8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goGQkKcwpLRJ"
      },
      "source": [
        "Como el Quijote es muy largo y tenemos muchas secuencias, podríamos encontrar problemas de memoria. Por ello, vamos a elegir un número máximo de ellas. Si estás corriendo esto localmente y tienes problemas de memoria, puedes reducir el tamaño aún más, pero ten cuidado porque, a menos datos, peor calidad del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pm1Q19ppw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7e8f9c-dd93-46f6-9ebc-3a369fb1ab2c"
      },
      "source": [
        "MAX_SEQUENCES = 350000\n",
        "\n",
        "sequences = np.array(sequences_filtradas)\n",
        "next_chars = np.array(next_chars_filtradas)\n",
        "\n",
        "perm = np.random.permutation(len(sequences_filtradas))\n",
        "sequences, next_chars = sequences[perm], next_chars[perm]\n",
        "sequences, next_chars = np.array(sequences), np.array(next_chars)\n",
        "sequences, next_chars = sequences[perm], next_chars[perm]\n",
        "sequences, next_chars = list(sequences[:MAX_SEQUENCES]), list(next_chars[:MAX_SEQUENCES])\n",
        "\n",
        "print(len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "350000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por factores de consumo de recursos de Google colab, no se utilizaran el 1.968.193 datos, sino que se utilizara una fraccion de estos, especificamente 350.000, donde el codigo genera con la permutacion un bajareo de indices de las secuencias y sus siguientes carateres, esto para tomar los 350.000 datos de manera aleatoria."
      ],
      "metadata": {
        "id": "hGNdxljAIuCg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FzgtAbPIs6f"
      },
      "source": [
        "#### 2.3. Obtención de input X y output y para el modelo\n",
        "\n",
        "Finalmente, a partir de los datos de entrenamiento que hemos generado vamos a crear los arrays de datos X e y que pasaremos a nuestro modelo.\n",
        "\n",
        "Para ello, vamos a utilizar *one-hot encoding* para nuestros caracteres. Por ejemplo, si sólo tuviéramos 4 caracteres (a, b, c, d), las representaciones serían: (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0) y (0, 0, 0, 1).\n",
        "\n",
        "De este modo, **X** tendrá shape *(num_sequences, seq_length, num_chars)* e **y** tendrá shape *(num_sequences, num_chars)*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMBwZ9obNGNg"
      },
      "source": [
        "NUM_CHARS = 35  # Tu número de caracteres distintos aquí\n",
        "NUM_SEQUENCES = len(sequences)\n",
        "X = np.zeros((NUM_SEQUENCES, SEQ_LENGTH), dtype=np.int32)\n",
        "y = np.zeros((NUM_SEQUENCES,), dtype=np.int32)\n",
        "\n",
        "\n",
        "for i, seq in enumerate(sequences):\n",
        "    for t, char in enumerate(seq):\n",
        "        X[i, t] = char_to_index[char]\n",
        "    y[i] = char_to_index[next_chars[i]]  # siguiente caracter (1 por secuencia)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este codigo genera nuestra separacion de datos en X e Y, utiliza el arreglo de caracteres a indices para guardar la secuencia en X (de manera numerica con el indice entero) y su siguiente carater en Y con el valor numerico ya listo para pasarse al modelo."
      ],
      "metadata": {
        "id": "obbTIHhUMbMs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeUxz3HPm3l"
      },
      "source": [
        "## 3. Definición del modelo y entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se tomo punto base el crear la LSTM con 128 unidades internas, utilizando softmax. Pero la arquitectura de la RNN se fue complejizando hasta dejar una arquitectura y preprocesamiento de datos mas completo buscando mejorar el rendimiento del modelo. Las tecnicas aplicadas como preprocesamiento y definicion del modelo son las siguientes:\n",
        "\n",
        "- Generacion de pesos artificiales: Esto para poder sustentar un poco el desbalance de datos, cuya existencia en un set de datos no estructurado es casi inevitable.\n",
        "\n",
        "- Aplicacion de Early Stopping: Debido al alto consumo de recursos para la RNN se aplico early stopping para evitar que el modelo siga consumiendo recursos en epocas que solo estan empeorando o manteniendo el mismo rendimiento.\n",
        "\n",
        "- Uso de LSTM junto a bidirectional con 512 unidades de memoria: Para mejorar el rendimiento general del modelo se aplica una capa LSTM con bidireccional para que el modelo procese la secuencia de entrada de izquierda a derecha (como loa hace la LSTM tradicional) pero a su vez aplique un analisis de derecha a izquierda. Se apicaron 512 unidades de memoria, en contra dle punto inicial debido al bajo rendimiento que se entrego con esa prueba."
      ],
      "metadata": {
        "id": "hSvPKrLBMrTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conteo = Counter(next_chars)\n",
        "total = sum(conteo.values())\n",
        "\n",
        "# Frecuencia relativa de cada carácter\n",
        "frecuencias = {char: count / total for char, count in conteo.items()}\n",
        "\n",
        "# Peso inverso para cada clase\n",
        "pesos = {char: 1.0 / freq for char, freq in frecuencias.items()}\n",
        "\n",
        "max_peso = max(pesos.values())\n",
        "pesos = {char: peso / max_peso for char, peso in pesos.items()}\n",
        "\n",
        "sample_weights = np.array([pesos[c] for c in next_chars], dtype=np.float32)"
      ],
      "metadata": {
        "id": "pHk_jZwhRdl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque de codigo genera pesos artificiales calculando la frecuencia del siguiente caracter usando Counter, con esto se aplican pesos inversos, es decir, los caracteres mas frecuentes se les genera un peso menor a los menos frecuentes buscando compensar el desbalance de clases claramente presente."
      ],
      "metadata": {
        "id": "wf5jC0s5QWQk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSw2j0btYWZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3375e760-f821-4f41-b365-0489b2e94fa1"
      },
      "source": [
        "embedding_dim = 512\n",
        "rnn_units = 512\n",
        "early_stopping = EarlyStopping(monitor=\"loss\", patience=5, min_delta=0.05, restore_best_weights=True)\n",
        "\n",
        "model_2 = Sequential([\n",
        "    Embedding(input_dim=NUM_CHARS, output_dim=embedding_dim),\n",
        "    Bidirectional(LSTM(rnn_units, return_sequences=True, dropout=0.2)),\n",
        "    Bidirectional(LSTM(rnn_units, return_sequences=False, dropout=0.2)),\n",
        "    Dense(NUM_CHARS, activation='softmax')\n",
        "])\n",
        "\n",
        "model_2.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy')\n",
        "model_2.fit(X, y, batch_size=64, epochs=15, callbacks=[early_stopping], sample_weight=sample_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 92ms/step - loss: 2.8719e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 93ms/step - loss: 2.1499e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 93ms/step - loss: 2.3667e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 93ms/step - loss: 2.1297e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 93ms/step - loss: 1.8381e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m5469/5469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 93ms/step - loss: 1.7012e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c3f02f71a50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se aplico una RNN que se compone de las siguientes capas:\n",
        "\n",
        "- Capa de embedding con dimesion de 512: Esto tomara los numeros enteros (los indices de los caracteres) y seran aplanados en un vector de 512 dimensiones. Esto puede mejorar bastante el rendimiento del modelo pero el consumo de recursos es mucho mayor\n",
        "\n",
        "- 2 capas LSTM con bidirectional y 512 unidades de memoria (neuronas): Se utilizaron dos capas LSTM envoltas en capas Bidirectional, lo que permite que la red procese la información tanto en dirección hacia adelante como hacia atrás, capturando mejor el contexto en ambas direcciones. La primera capa devuelve una secuencia completa de características (una salida por cada paso de la secuencia), que luego es procesada por la segunda capa para extraer patrones más complejos de lenguaje. Esto mejora la capacidad del modelo para aprender dependencias a largo plazo en el texto.\n",
        "\n",
        "- Capa densa de salida: La capa que nos entregara el siguiente caracter de la secuencia, como sucede en las redes de clasificacion tradicionales.\n",
        "\n",
        "- Funcion de perdida: Se aplica Sparse_categorical_crossentropy debido a que no se esta utilizando un formato de one hot encoder, se utiliza esa funcion de perdida.\n"
      ],
      "metadata": {
        "id": "veRvahgKQi62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_multiple_texts(model, start_phrases, gen_length=100, temperature=1.0):\n",
        "    for phrase in start_phrases:\n",
        "        input_indices = [char_to_index[c] for c in phrase.lower()]\n",
        "        input_seq = tf.expand_dims(input_indices, 0)\n",
        "        generated = phrase\n",
        "\n",
        "        for _ in range(gen_length):\n",
        "            preds = model(input_seq)  # última predicción\n",
        "            preds = preds / temperature\n",
        "            next_id = tf.random.categorical(preds, num_samples=1).numpy()[0][0]\n",
        "            next_char = index_to_char[next_id]\n",
        "\n",
        "            generated += next_char\n",
        "            input_indices.append(next_id)\n",
        "            input_indices = input_indices[-SEQ_LENGTH:]\n",
        "            input_seq = tf.expand_dims(input_indices, 0)\n",
        "\n",
        "        print(f\"\\n🟡 Frase inicial: '{phrase}'\")\n",
        "        print(f\"🔹 Predicción: {generated}\")"
      ],
      "metadata": {
        "id": "QZz072k6E4vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código define una función llamada generate_multiple_texts que se encarga de generar texto automáticamente usando un modelo de red neuronal entrenado. Lo que hace, explicado en lenguaje natural, es lo siguiente:\n",
        "\n",
        "La función toma una lista de frases iniciales (start_phrases) y, para cada una, empieza a predecir y generar texto carácter por carácter. Primero convierte la frase de entrada en una secuencia de números (índices de caracteres) que el modelo pueda entender. Luego, mientras no se alcance la cantidad deseada de caracteres (gen_length), la función alimenta esa secuencia al modelo, que devuelve una probabilidad para cada posible carácter siguiente.\n",
        "\n",
        "Estas probabilidades se ajustan con un valor llamado temperature, que controla qué tan creativas o seguras son las predicciones (valores bajos = más conservadoras; valores altos = más creativas y variadas). Después, se escoge un carácter de forma aleatoria pero ponderada según esas probabilidades, se agrega a la secuencia y se repite el proceso, desplazando la ventana de entrada para siempre usar los últimos caracteres generados.\n",
        "\n",
        "Al final, se imprime tanto la frase original como el texto generado automáticamente por el modelo, mostrando cómo continúa la red neuronal esa idea inicial."
      ],
      "metadata": {
        "id": "_1MNKlU1zFAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frases_prueba = [\n",
        "    \"don quijote \",\n",
        "    \"en un lugar de la \",\n",
        "    \"no ha mucho tiempo \",\n",
        "    \"vivia un hidalgo \",\n",
        "    \"de los de lanza en \",\n",
        "    \"era de complexion \",\n",
        "    \"el hidalgo salio al \",\n",
        "    \"andaba por los caminos \",\n",
        "    \"una mañana decidio \"\n",
        "]\n",
        "\n",
        "generate_multiple_texts(model_2, frases_prueba, gen_length=60, temperature=0.5)"
      ],
      "metadata": {
        "id": "lesg4Q9F4Fys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8d46ba-dd4a-49bb-e794-7592d785dd77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟡 Frase inicial: 'don quijote '\n",
            "🔹 Predicción: don quijote s1fgyavcyt5w1lmicgb7lgorcgxbl56qamfuoazummv6so0d1xjv0 vmoeñw\n",
            "\n",
            "🟡 Frase inicial: 'en un lugar de la '\n",
            "🔹 Predicción: en un lugar de la quubgy11ñuujzou7vñpbd25dañoqs6l00ozdn3gwz2x0wecggmjs2ynñev w\n",
            "\n",
            "🟡 Frase inicial: 'no ha mucho tiempo '\n",
            "🔹 Predicción: no ha mucho tiempo pjhltlclyba4u2d72icz45ife34rafpantdnaf0rnn27qltoq0mocg36jdod\n",
            "\n",
            "🟡 Frase inicial: 'vivia un hidalgo '\n",
            "🔹 Predicción: vivia un hidalgo en2y3fu4rhfjñyrj0eñwe svqu3xe7a7ayx607j1w0dsy v0c7 m4 31b45y\n",
            "\n",
            "🟡 Frase inicial: 'de los de lanza en '\n",
            "🔹 Predicción: de los de lanza en ytglbyalqlwnqb1rxpanjgisñr754grbotyzm 4 aqubn6wdiñ7ha6g4iryn\n",
            "\n",
            "🟡 Frase inicial: 'era de complexion '\n",
            "🔹 Predicción: era de complexion a1r7eyñaafllaf2cñf0ren zpgtzeoivhl4bodo12 yyvsxedtuie5ap06wn\n",
            "\n",
            "🟡 Frase inicial: 'el hidalgo salio al '\n",
            "🔹 Predicción: el hidalgo salio al 3lslquawbeoelq6rx4rczujhy n5a6hjbmnplxy7nrsteqogyyarb6wqc7j7\n",
            "\n",
            "🟡 Frase inicial: 'andaba por los caminos '\n",
            "🔹 Predicción: andaba por los caminos 7u5ñ m6z25qyv2lzzlcmr27ñluf3pri6z4topeu04i7fb7yrm6g6qñzozwvo\n",
            "\n",
            "🟡 Frase inicial: 'una mañana decidio '\n",
            "🔹 Predicción: una mañana decidio o3yrjir4fi1wd5qofspñeadu3al4mvyqñwmm 4nt6i1q0wx4ñ7zsrn1axsob\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui se le entrega una lista de frases a la funcion para que el model orealice multiples predicciones, esto permite evaluar de forma general y evitar evaluar el modelo con una frase que pueda llegar a ser compleja por el rendimiento del mismo."
      ],
      "metadata": {
        "id": "grq0vf2L0LOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusión"
      ],
      "metadata": {
        "id": "0V9KJzcg0Wqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido a los recursos limitados que posee el entorno de ejecucion el rendimiento general del modelo es bastante bajo llegando al punto de que las predicciones no forman una palabra coherente (lenguaje español). Esto puede deberse a la capa de embedding, debido a que vectoriza los datos en una dimension de 512 se necesita un mayor consumo de recursos, los cuales no se poseen. Cabe destacar que se realizara un estudio tomando un enfoque diferente donde se pueda realizar un preprocesamiento de datos utilizando formato one-hot-encoding, lo cual permitira llevarlo a un enfoque mas de multi clases con matrices de 0 y 1, esto hara que la capa de embedding no sea necesaria y el modelo no tenga que consumir tantos recursos para procesarlo dando un margen de recursos para poder distribuir de mejor forma y mejorar asi el rendimiento del mismo.\n",
        "\n",
        "Enlace a investigacion con one-hot-encoding (Notebook 2):"
      ],
      "metadata": {
        "id": "eqTwF4pD0aIT"
      }
    }
  ]
}